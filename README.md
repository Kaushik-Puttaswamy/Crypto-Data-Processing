# Crypto Data Processing and Analysis Pipeline Using AWS Glue, Kinesis, DynamoDB, Athena and QuickSight

## ğŸ“Œ Project Overview

The Crypto Data Processing Pipeline is an end-to-end data processing solution designed to handle cryptocurrency transactions in real time. It leverages AWS Glue, AWS Lambda, Kinesis Firehose, DynamoDB Streams, and Hudi for efficient ETL and analytics. The project processes raw transaction data, performs transformations, categorizes trades based on risk, and stores processed data in an S3-based Hudi table for further analysis uisng AWS QucikSight.

â€¢ Real-time data ingestion from DynamoDB streams

â€¢ CDC (Change Data Capture) processing with Kinesis Firehose

â€¢ Spark-based ETL with AWS Glue and Apache Hudi

â€¢ Data lake storage with S3 and AWS Glue Catalog

â€¢ Interactive analytics with Athena and QuickSight

## ğŸ“‚ Project Structure

Crypto-Data-Processing/

â”‚â”€â”€ crypto_nrt_etl_glue_job.py               # AWS Glue ETL script for data transformation & storage

â”‚â”€â”€ lambda_transformer.py                     # AWS Lambda function for processing DynamoDB Streams

â”‚â”€â”€ mock_crypto_data_to_dynamodb.py           # Script to simulate crypto transactions and insert them into DynamoDB

â”‚â”€â”€ Project Execution Screenshots/            # Folder containing setup and execution screenshots

â”‚â”€â”€ Project architecture                      # High-level architecture and data flow

â””â”€â”€ README.md                                 # Project documentation (this file)

## âœ¨ Key Features

â€¢ Real-Time Risk Analysis: Flag high-value trades (>$500k)

â€¢ Multi-Exchange Normalization: Price adjustments per exchange

â€¢ Dynamic Fee Calculation: Volume-based fee discounts

â€¢ Time-Bucket Aggregation: Hourly trade analysis

â€¢ ACID Compliance: Apache Hudi for UPSERT operations

## âœ…  Prerequisites

Before setting up the Crypto Data Processing and Analysis Pipeline, ensure you have the following:

1. AWS Account & Services
	
 	â€¢ An AWS Account with access to:
	
 	â€¢ AWS Glue (for ETL and data transformation)
	
 	â€¢ AWS DynamoDB (for storing raw crypto transactions)
	
 	â€¢ AWS Kinesis Firehose (for real-time streaming)
	
 	â€¢ AWS Lambda (for data transformation)
	
 	â€¢ AWS S3 (for storing raw and processed data)
	
 	â€¢ AWS Athena (for querying processed data)
	
 	â€¢ AWS QuickSight (for visualizing insights)

2. AWS CLI & SDKs
	
 	â€¢ Install and configure AWS CLI:

	```pip install awscli```

	```aws configure``` 

3. Python & Dependencies
	
	â€¢ Install Python 3.x and required libraries:

	```pip install faker``` 

	```pip install boto3``` 

4. Permissions & IAM Roles

	â€¢ Create IAM roles with the following policies:

	â€¢ Glue Service Role (AWSGlueServiceRole)

	â€¢ Lambda Execution Role (AWSLambdaBasicExecutionRole)
	
	â€¢ Kinesis Firehose Access Policy

	â€¢ DynamoDB Stream Access Policy

5. AWS S3 Bucket for Storage
	
	â€¢ Create an S3 bucket for storing raw and processed data.

6. Enable DynamoDB Streams

	â€¢ Enable DynamoDB Streams on the CryptoTransactions table for real-time data ingestion.

## ğŸ› Project Architecture

![Project Architecture .png](https://github.com/Kaushik-Puttaswamy/Crypto-Data-Processing-and-Analysis-Pipeline-Using-AWS-Glue-Kinesis-DynamoDB-Athena-and-QuickSight/blob/main/Project%20Architecture%20.png)

### Data Flow:

â€¢ 1, Mock data generator â†’ DynamoDB

â€¢ 2, 3 DynamoDB Stream â†’ Kinesis Firehose

â€¢ 4, Firehose â†’ Lambda (Transformation)

â€¢ 5, 6, 7, 8, 9 Raw S3 â†’ Glue ETL (Hudi Processing)

â€¢ 10, 11 Processed S3 â†’ Athena â†’ QuickSight


This project follows a real-time data ingestion and processing pipeline:
	
1. Data Generation (Mock Transactions)

	â€¢ A Python script (mock_crypto_data_to_dynamodb.py) continuously generates mock crypto transactions.

	â€¢ The data is inserted into AWS DynamoDB.

2. Data Streaming & Transformation

	â€¢ DynamoDB Streams captures changes and forwards them to AWS Kinesis Firehose.

	â€¢ A Lambda function (lambda_transformer.py) transforms the data from DynamoDBâ€™s format to JSON.
	
3. Real-Time ETL Processing (AWS Glue)

	â€¢ AWS Glue reads data from S3 (landing zone).

	â€¢ Performs data transformations, including:

	â€¢ Risk flagging based on trade value.

	â€¢ Price normalization across exchanges.

	â€¢ Fee adjustments based on transaction size.

	â€¢ User categorization based on trade volume.

	â€¢ Writes processed data into an AWS S3-based Hudi table.
	
4. Data Storage & Analytics

	â€¢ Processed Data: Stored in an S3 Hudi table (processed_crypto_txn).

	â€¢ Querying: AWS Athena is used to query Hudi tables.

	â€¢ Visualization: QuickSight dashboards are built on Athena queries.

## ğŸ›  Setup & Execution Steps

Step 1: Configure AWS DynamoDB

â€¢ Create a DynamoDB Table (CryptoTransactions).

â€¢ Enable DynamoDB Streams to capture changes.

ğŸ“Œ Screenshot: DynamoDB_Table_Setup_with_Data_stream_and_CDC_enabled.png

Step 2: Setup AWS Kinesis Firehose
	
 â€¢ Create a Kinesis Firehose delivery stream.
	
 â€¢ Configure Firehose to read from DynamoDB Streams and deliver data to S3.

ğŸ“Œ Screenshot: Firehose_Stream_Setup.png

ğŸ“Œ Screenshot: Kinesis_Setup_with_Shard_Showing_Records.png

Step 3: Create & Deploy AWS Lambda Function

â€¢ Deploy lambda_transformer.py to process data from Kinesis Firehose.

â€¢ This function:
	
 â€¢ Converts DynamoDB JSON format into standard JSON.
	
 â€¢ Adds metadata (event type, timestamp, event ID).
	
 â€¢ Outputs transformed data to Kinesis Firehose.

ğŸ“Œ Screenshot: Lambda_Function_for_Data_Transformation.png

ğŸ“Œ Screenshot: Lambda_Function_Permission_Policies.png


Step 4: Setup AWS Glue for ETL
	
 â€¢ Create an AWS Glue Crawler to catalog data from S3 (Raw Transactions).
	
 â€¢ Define a Glue ETL Job to process data using crypto_nrt_etl_glue_job.py.
	
 â€¢ Glue ETL performs:
	
 â€¢ Data Transformation
	
 â€¢ Trade Risk Classification
	
 â€¢ Price Normalization
	
 â€¢ User Categorization
	
 â€¢ Hudi-based storage in S3


ğŸ“Œ Screenshot: Glue_Database_Setup.png

ğŸ“Œ Screenshot: Glue_Table_for_Raw_Processed_Data.png

ğŸ“Œ Screenshot: Glue_ETL_Setup.png

ğŸ“Œ Screenshot: Glue_ETL_Job_Status.png

Step 5: Query Data using AWS Athena
	
 â€¢ AWS Athena is used to run SQL queries on processed crypto transactions stored in Hudi tables.
	
 â€¢ Sample query:


``` select * from crypto.processed_crypto_txn limit 10; ```

``` select count(*) from crypto.processed_crypto_txn limit 10; ```

``` SELECT * FROM crypto.processed_crypto_txn WHERE risk_flag = 'MEDIUM_RISK'; ```

ğŸ“Œ Screenshot: Athena_Query_Execution_for_Processed_Data.png

Step 6: Visualize Data using QuickSight
	
 â€¢ AWS QuickSight connects to Athena and visualizes trade risk trends, price normalization, and user activity.
	
 
 Insights include:

 â€¢ Trade Fee Collected
 
 â€¢ Exchange Comparison
	
 â€¢ Trading Volume per Trading Pair
	
 â€¢ Risk Distribution
	
 â€¢ Number of Trades by Status
 
 â€¢ Order Type Distribution
 
 â€¢ User Category Distribution

ğŸ“Œ Screenshot: Crypto_Trading_Insights_Dashboard_using_QuickSight.png

link: https://us-east-1.quicksight.aws.amazon.com/sn/dashboards/78dd677e-34d1-4a4f-9b78-82a2f97863ba/views/fe261a5f-630c-404e-ac18-8429d246000e?directory_alias=DataEngineering-QuickSight-Dashboard

ğŸ“Œ Screenshot: QuickSight_File_Screenshot.png

Step 7: Automate Execution with AWS Triggers
	
 â€¢ Set up AWS Glue triggers to run ETL jobs automatically based on new data arrival.

ğŸ“Œ Screenshot: Trigger_Setup.png

## ğŸ”¥ Key Features & Enhancements

âœ… Real-Time Processing: Uses DynamoDB Streams + Kinesis Firehose for event-driven processing.

âœ… Risk-Based Classification: Flags transactions based on trade value.

âœ… Price Normalization: Adjusts price across multiple exchanges for fair comparison.

âœ… User Categorization: Identifies VIP Traders, Active Traders, and Casual Traders.

âœ… Athena & QuickSight Analytics: Enables ad-hoc analysis and business intelligence dashboards.

âœ… Efficient Storage with Hudi: Ensures incremental updates & faster queries using Apache Hudi.

## ğŸš€ How to Run Locally?

Step 1: Install Dependencies

Ensure you have Python 3.x and the required AWS libraries:

``` pip install boto3 faker ```

Step 2: Simulate Transactions (Mock Data)

Run the script to insert transactions into DynamoDB:

``` python mock_crypto_data_to_dynamodb.py ```

Step 3: Deploy AWS Resources
	
 1.	Deploy Lambda function (lambda_transformer.py).
	
 2.	Configure Kinesis Firehose and connect it to DynamoDB Streams.
	
 3.	Setup Glue Crawler & Glue ETL Job.

Step 4: Run AWS Glue ETL

Manually trigger the Glue ETL job:

``` aws glue start-job-run --job-name crypto_nrt_etl_glue_job ```

Step 5: Query & Analyze Data

Use AWS Athena or QuickSight to analyze processed data.

## ğŸ“Œ Future Enhancements


ğŸ”¹ Add Machine Learning Anomaly Detection for fraud detection.

ğŸ”¹ Implement Kafka instead of Kinesis for higher scalability.

ğŸ”¹ Support Multi-Region Data Processing for global transaction tracking.

ğŸ”¹ Enhance Data Governance with AWS Lake Formation.

## ğŸ‘¤ Project Author
	
 â€¢ Kaushik Puttaswamy  - Aspiring Data Engineer (https://www.linkedin.com/in/kaushik-puttaswamy-data-analyst/)

ğŸ“§ For queries, reach out at ```kaushik.p9699@gmail.com```

## ğŸ¯ Conclusion

This project demonstrates real-time crypto transaction processing using AWS Glue, DynamoDB, and Apache Hudi. The system is scalable, efficient, and provides insights into high-risk transactions, exchange price variations, and trading behaviors.

ğŸš€ Now youâ€™re ready to build a robust data pipeline for crypto analytics! ğŸ‰

